{
    "questions": [
        {
            "id": "1",
            "question": "Explain how the use of tokenizers in Hugging Face affects the performance of models when dealing with text data.",
            "template": "The candidate should explain tokenization as the process of converting text into numerical representations, which are then used as inputs to the model. They should then discuss different tokenization methods like WordPiece, BPE, and SentencePiece, and how they impact the way text is represented (sub-word vs word) and thereby model performance, including how they handle out-of-vocabulary (OOV) words. They should explain that different tokenization strategies can affect the number of tokens generated, which can influence memory consumption and computational costs. They need to also discuss how the choice of tokenizer can impact the model's ability to capture the nuances of the language, the vocabulary size, and generalization capacity.",
            "criteria": "The answer should demonstrate a clear understanding of tokenization, including the function and how different tokenization techniques (WordPiece, BPE, SentencePiece) work. The answer should clearly state the relationship between the tokenization strategy and its effects on memory, computational costs and the model\u2019s performance, including vocabulary size and generalization. The explanation should have a logical structure and be easy to understand.",
            "category": "Theoretical"
        },
        {
            "id": "2",
            "question": "What is the role of 'intent' and 'entities' in the context of building a chatbot using Dialogflow?",
            "template": "The candidate should describe intents as representations of the user's goal or purpose behind their input and entities as specific pieces of information the user provides. They should articulate that intents enable the chatbot to understand the type of request a user is making (e.g., 'book a flight,' 'check the weather') and entities are used to extract specific details necessary for fulfilling that request (e.g., 'destination,' 'date'). The explanation should include examples to differentiate between the roles of intents and entities. The candidate should demonstrate an understanding that entities are extracted based on the intent, and that this helps in creating more directed and contextually appropriate responses. They should show how the combination of intents and entities enables the chatbot to perform appropriate actions or provide relevant responses.",
            "criteria": "The answer must correctly define intents as the user's intention and entities as specific pieces of information. The answer must demonstrate how intents and entities work together to help the chatbot respond appropriately to users, showing clear examples of each. The explanation should be precise, coherent, and demonstrate a functional understanding of Dialogflow.",
            "category": "Theoretical"
        },
        {
            "id": "3",
            "question": "Explain how real-time data synchronization in Firebase Realtime Database contributes to live messaging functionality.",
            "template": "The candidate should explain that Firebase Realtime Database uses a cloud-hosted NoSQL architecture which automatically syncs data across connected clients in real time. They need to describe that any change made to the data in the database immediately triggers an update across all connected clients. They should emphasize how the instant synchronization of messages is essential for creating real-time interactive user experience in chat applications, and how it ensures that messages sent by a user are instantly visible to all other active clients/participants of the conversation. The response should highlight the significance of this functionality in maintaining consistent and immediate information across multiple users and devices during live message interactions. They should briefly touch upon how data is structured as JSON in a NoSQL database and how that helps in live data sync.",
            "criteria": "The answer must demonstrate a clear understanding of real-time data synchronization, specifically using Firebase Realtime Database. It should explain how real-time synchronization enables live messaging by ensuring instant data updates across connected clients. The candidate must explain that the database syncs only the delta changes, and is a NoSQL database structure. The answer should be clear, concise, and showcase the candidate's grasp on real-time data functionalities and its use in chat applications.",
            "category": "Theoretical"
        },
        {
            "id": "4",
            "question": "Explain the differences between a DataFrame and a Resilient Distributed Dataset (RDD) in PySpark, focusing on their structure, immutability, and typical use cases.",
            "template": "The candidate should explain that DataFrames are structured data with named columns, offering more optimization opportunities. RDDs are distributed collections of data that are immutable and serve as a more general-purpose primitive. They should also discuss that RDDs are typically used for low-level transformations and DataFrames for structured data analysis and querying.",
            "criteria": "The response should accurately differentiate DataFrames and RDDs, mentioning their structure, immutability, and use cases. The candidate should demonstrate an understanding of the performance benefits of using DataFrames when applicable.",
            "category": "Technical Skills"
        },
        {
            "id": "5",
            "question": "Describe the process of building a data pipeline using Apache Airflow. Include how you would define tasks, dependencies, and handle failures.",
            "template": "The candidate should explain that tasks are defined using Python operators. Dependencies are managed using directed acyclic graphs (DAGs). They should also discuss that error handling can be done using retry mechanisms and alerts. They should demonstrate an understanding of how to define and execute tasks, and manage dependencies in a workflow",
            "criteria": "The answer should describe how tasks are defined in Airflow using operators, the process of setting dependencies with DAGs and the mechanism for handling task failures. The explanation should be clear and indicate practical experience with Airflow concepts",
            "category": "Technical Skills"
        },
        {
            "id": "6",
            "question": "How would you implement a change data capture (CDC) process for a transactional database to load data into a Delta Lake table?",
            "template": "The candidate should explain that CDC can be done using tools like Debezium or by reading transaction logs. The incremental changes should be applied to the Delta Lake table using merge operations. They should describe methods for identifying changed records and applying updates correctly using the merge operation's 'on' condition and 'when matched/not matched' clauses.",
            "criteria": "The answer should cover the concept of CDC, methods for extracting changes, and how to update Delta Lake using `merge`. It should demonstrate understanding of how data from a transactional database can be incrementally loaded and updated in a Delta Lake with consistency.",
            "category": "Technical Skills"
        },
        {
            "id": "7",
            "question": "What are the primary considerations when designing a data model for a star schema in a data warehouse environment?",
            "template": "The candidate should outline the dimensions of the data, the fact table, and considerations for key selection (primary key, foreign keys), granularity, and ensuring minimal redundancy. They should also explain the tradeoffs for creating conformed dimensions for reusability and ease of analysis. They should showcase a good understanding of data warehousing concepts.",
            "criteria": "The answer should cover the core elements of a star schema (fact and dimensions), and explain various modeling considerations such as key selection and granularity. The candidate should demonstrate strong conceptual understanding of dimensional modeling for a data warehouse.",
            "category": "Technical Skills"
        },
        {
            "id": "8",
            "question": "Explain different types of joins in SQL. Provide a scenario to demonstrate when you would prefer one join over another.",
            "template": "The candidate should describe inner join, left join, right join, full outer join. They should explain how they function and when would they be most useful for a given scenario. For example, using an inner join when only wanting matching records, and a left join when you need all records from the left side. They should showcase the understanding of different types of joins with clear examples.",
            "criteria": "The answer should accurately explain the four different types of joins in SQL. The candidate should be able to describe when they would use a certain join over the other. Understanding the scenario usage is essential to fulfilling the criteria.",
            "category": "Technical Skills"
        },
        {
            "id": "9",
            "question": "How can you optimize a slow-performing SQL query? Describe techniques to identify bottlenecks and implement performance improvements.",
            "template": "The candidate should discuss techniques like using EXPLAIN plans to identify bottlenecks, using indexes effectively, optimizing joins, avoiding full table scans, and using appropriate data types. They should show the understanding of query performance analysis with specific techniques and implementation.",
            "criteria": "The answer should describe using explain plans, indexes and appropriate join optimization techniques, along with appropriate data type usage and avoiding full table scans. They should provide examples of how each technique would improve the query performance.",
            "category": "Technical Skills"
        },
        {
            "id": "10",
            "question": "How would you ensure data quality during the ETL process, including specific checks and validation steps you would implement?",
            "template": "The candidate should discuss adding data validation steps, such as schema validation, null checks, range checks, data type validation, and implementing rules and constraints. They should explain how to handle invalid data (e.g., logging, quarantine, or transformation) and ensure data consistency and validity throughout the process. They should showcase the practical approaches to ensuring data integrity in an ETL pipeline",
            "criteria": "The answer should include multiple data validation techniques during ETL. It should include both data type and constraint checks. They should demonstrate the ability to explain the process and the criteria required to maintain consistent data integrity and accuracy.",
            "category": "Technical Skills"
        },
        {
            "id": "11",
            "question": "Discuss the advantages of using Delta Lake over traditional data lakes. Include aspects like ACID properties, time travel, and schema evolution.",
            "template": "The candidate should discuss how Delta Lake provides ACID properties (atomicity, consistency, isolation, durability), enabling reliable data modifications. They should also cover the time travel feature, and the ability to evolve schemas while maintaining compatibility. The candidate should be able to discuss each property with suitable example.",
            "criteria": "The response should clearly discuss the advantages of Delta Lake focusing on how ACID properties and time travel helps in maintaining reliable data modification, and how Delta Lake has schema evolution capabilities compared to normal data lakes.",
            "category": "Technical Skills"
        },
        {
            "id": "12",
            "question": "Describe your approach to version controlling data pipelines using Git. Explain how you would handle branching, merging, and deploying changes effectively.",
            "template": "The candidate should discuss using Git for tracking changes to pipeline code, including configuration files. They should explain a workflow using branches for features, pull requests for code reviews, and the process of merging changes and deploying to different environments (e.g., development, staging, production). They should showcase their understanding of using version control to manage data pipeline changes",
            "criteria": "The response should showcase how to use Git for version controlling of data pipeline code, and workflows. They should also provide the use case of deploying the code to different environments (like development and production). They should also show the practical understanding with usage of branching and merging.",
            "category": "Technical Skills"
        },
        {
            "id": "13",
            "question": "How do you approach data security in a cloud environment when designing a data pipeline? Describe various security considerations and best practices.",
            "template": "The candidate should discuss data encryption in transit and at rest, role based access control (RBAC), network segmentation, secure credentials management, monitoring and auditing and usage of a proper secrets management strategy. They should also touch on cloud security best practices and maintaining compliance.",
            "criteria": "The answer should provide comprehensive overview of the data security aspects for a data pipeline on the cloud, including encryption, RBAC and proper secrets management. They should be aware of the compliance requirements and cloud security best practices.",
            "category": "Technical Skills"
        },
        {
            "id": "14",
            "question": "You're working on a critical data pipeline that's experiencing unexpected latency issues. Several stakeholders from different teams are relying on this pipeline, and you need to diagnose the problem and implement a solution quickly. How would you approach investigating and resolving this issue while keeping all stakeholders informed?",
            "template": "The candidate should demonstrate a structured approach to problem-solving, emphasizing effective communication and collaboration. This includes: 1. Quickly assess the scope of the issue. 2. Formulate a hypothesis for potential causes of the latency. 3. Use available tools (monitoring dashboards, logs) to gather evidence. 4. Prioritize the debugging effort based on data. 5. Communicate the ongoing diagnostics with key stakeholders. 6. Propose a solution based on the findings. 7. Implement and test the solution. 8. Share a post-mortem analysis of the event.",
            "criteria": "The response should exhibit clear problem-solving steps and excellent communication skills. The candidate should display a sense of urgency while maintaining a methodical process. Successful responses will detail the tools and methodologies the candidate has used, and how these were deployed in an efficient way. They will prioritize clear and continuous communication with stakeholders throughout each step.",
            "category": "Situational"
        },
        {
            "id": "15",
            "question": "In your experience working on the Legal Argument Reasoning project for American Civil Procedure, you achieved a 60% increase in Macro F1 score. Describe a situation where the initial approach you tried didn't yield the expected results, and how you pivoted to achieve that exceptional performance.",
            "template": "The candidate should provide a specific instance where their initial attempt or strategy did not produce optimal results and the way they adapted their approach. This includes: 1. Clearly describing the initial approach. 2. Analyzing the reasons behind why it did not work. 3.  Showcasing the alternative solution they came up with. 4.  Highlighting their thought process in transitioning to the new approach. 5. Emphasizing the outcome of the pivot and its contribution to the project success. 6. Discussing lessons learned.",
            "criteria": "The response must demonstrate the candidate's ability to analyze failure, adapt, and learn from challenges. Successful responses will emphasize critical thinking, problem-solving, and resourcefulness. They should show a clear understanding of their approach, the ability to identify weaknesses in their plan, and the proactiveness to shift towards a solution that works better. The ability to analyze and learn from failed attempts is a crucial part of their skillset.",
            "category": "Situational"
        }
    ]
}