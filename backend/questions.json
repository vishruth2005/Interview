{
    "questions": [
        {
            "id": "1",
            "question": "You've worked with RAG systems. Describe a scenario where using sentence transformers for embedding retrieval would be significantly more effective than traditional TF-IDF or BM25, and explain why. Detail the trade-offs in terms of computational cost and memory usage.",
            "template": "The candidate should explain the advantages of sentence transformers in capturing semantic similarity, especially when dealing with paraphrasing, synonyms, and context. They should provide a concrete example, such as retrieving information about 'customer complaints' when the query is 'negative feedback'. The discussion should also cover the higher computational cost of sentence transformers during both indexing and querying, as well as the increased memory requirements for storing embeddings compared to sparse TF-IDF representations. They should suggest strategies for mitigating these costs, such as using smaller embedding dimensions or approximate nearest neighbor search techniques.",
            "criteria": "The answer should demonstrate a clear understanding of the strengths and weaknesses of different retrieval methods in RAG systems. The candidate should articulate a practical scenario highlighting the benefits of sentence transformers and acknowledge the computational trade-offs. Bonus points for mentioning specific sentence transformer models (e.g., Sentence-BERT, all-mpnet-base-v2) and their performance characteristics.",
            "category": "Project Based"
        },
        {
            "id": "2",
            "question": "You've built knowledge graphs using RDF. Imagine you need to integrate a new dataset with a different ontology into your existing knowledge graph. Describe your approach to aligning the ontologies, handling potential conflicts in terminology and relationships, and ensuring data consistency after the integration.",
            "template": "The candidate should discuss techniques for ontology alignment, such as finding equivalent classes and properties using string-based similarity measures, logical reasoning, or machine learning-based methods. They should explain how to address potential conflicts in terminology (e.g., different terms referring to the same concept) and relationships (e.g., different ways of representing the same connection between entities). They should also describe strategies for ensuring data consistency, such as using SPARQL CONSTRUCT queries to transform the data to conform to the target ontology and employing data validation rules to detect and resolve inconsistencies. The candidate should also touch upon the importance of version control and provenance tracking during the integration process.",
            "criteria": "The answer should demonstrate a solid understanding of ontology alignment and data integration challenges in knowledge graphs. The candidate should present a systematic approach to handling terminology conflicts, relationship mismatches, and data inconsistencies. Bonus points for mentioning specific tools or techniques for ontology alignment (e.g., Prot\u00e9g\u00e9, OWL API) and data validation (e.g., SHACL).",
            "category": "Project Based"
        },
        {
            "id": "3",
            "question": "You have experience evaluating LLMs and their performance in complex query environments. Describe a scenario where you need to evaluate the effectiveness of Llama 3 8B (or a similar LLM) in a Databricks environment for question answering over a large Delta Lake table containing customer support tickets. What specific metrics would you use to assess its performance, and how would you measure the trade-off between accuracy, latency, and cost in this context?",
            "template": "The candidate should outline an experimental setup for evaluating the LLM's performance in answering questions over the Delta Lake table. They should mention pre-processing steps like indexing the data and structuring the queries. For metrics, they should discuss accuracy metrics like F1 score on answer span overlap with ground truth (if available), or semantic similarity scores (e.g., BERTScore) if relying on generative answers. They should also emphasize the importance of latency, measured in terms of query response time, and cost, which would involve considering Databricks compute resources and the LLM API usage costs (if applicable). The candidate should suggest strategies for analyzing the trade-offs, such as plotting accuracy vs. latency for different configurations (e.g., batch size, number of GPU workers) and estimating the total cost for each configuration. Bonus points for discussing the challenges of evaluating LLMs in real-world settings and potential mitigation strategies, such as using human evaluation or synthetic benchmarks.",
            "criteria": "The response should demonstrate a practical understanding of LLM evaluation in a real-world data setting. The candidate should suggest appropriate metrics for assessing accuracy, latency, and cost, and they should articulate a clear strategy for analyzing the trade-offs between these factors. The candidate should also demonstrate awareness of the practical challenges of LLM evaluation and potential solutions.",
            "category": "Project Based"
        },
        {
            "id": "4",
            "question": "Explain how Retrieval-Augmented Generation (RAG) enhances the performance of Language Models in tasks like vulnerability analysis.",
            "template": "The candidate should explain the basic principle of RAG: retrieving relevant information from a knowledge source and using it to augment the LLM's generation process. They should then elaborate on how using OWASP data in RAG improves vulnerability analysis, specifically mentioning that it allows the LLM to incorporate up-to-date and specific information, mitigating issues like hallucination and improving the accuracy and relevance of the generated outputs.",
            "criteria": "The answer should demonstrate a clear understanding of RAG, its benefits, and how it is applied in the context of vulnerability analysis with OWASP data. Key indicators are understanding of information retrieval, knowledge injection, and accuracy improvements in LLM outputs.",
            "category": "Theory Based"
        },
        {
            "id": "5",
            "question": "What is the role of knowledge graphs in question-answering systems, and how does building comprehensive knowledge graphs using RDF with ontology and data layers enable multi-level query generation?",
            "template": "The candidate should describe how knowledge graphs represent information, allowing for structured querying and reasoning. They should explain how RDF provides a standard for representing the graph structure and how ontologies define the concepts and relationships within the knowledge domain. They should also explain how ontology and data layers allow for querying across different levels of abstraction to get precise answers.",
            "criteria": "The response should cover the role of knowledge graphs in QA, the importance of RDF for structuring the graph, and the benefits of ontology and data layers for multi-level query generation. Good answers will explain how this approach improves the accuracy and expressiveness of the QA system.",
            "category": "Theory Based"
        },
        {
            "id": "6",
            "question": "Explain the concept of Transfer Learning and its benefits in Low-Resource NLP scenarios, specifically mentioning how fine-tuning custom pre-trained transformers like ALBERT and RoBERTa contributes to enhanced retrieval efficiency.",
            "template": "The candidate should first explain what transfer learning is: leveraging knowledge gained from training on a large dataset to improve performance on a smaller, related dataset. They should then explain why this is particularly beneficial in low-resource settings. Finally, the candidate should connect this to fine-tuning pre-trained transformers like ALBERT and RoBERTa, stating how this allows the model to adapt to the specific characteristics of the low-resource data and improve retrieval efficiency by leveraging the transformers' existing knowledge of language.",
            "criteria": "The response should demonstrate understanding of transfer learning, its importance in low-resource NLP, and the role of fine-tuning in adapting pre-trained models like ALBERT and RoBERTa for improved retrieval efficiency. The response should also reflect understanding of NLP tasks.",
            "category": "Theory Based"
        },
        {
            "id": "7",
            "question": "Explain your approach to handling imbalanced datasets when building a classification model using scikit-learn. Include specific techniques you've used and why they are effective. Furthermore, elaborate how these techniques would be applicable in a fraud detection scenario.",
            "template": "The candidate should demonstrate understanding of imbalanced datasets and their impact on model performance. They should mention techniques like oversampling (e.g., SMOTE), undersampling, cost-sensitive learning, and ensemble methods. For the fraud detection scenario, they should explain how these techniques can help the model better identify fraudulent transactions despite their low frequency compared to legitimate transactions.",
            "criteria": "The answer should demonstrate a clear understanding of the problem of imbalanced datasets. The candidate should showcase their knowledge of at least two relevant techniques with proper justification and appropriate scenarios. The fraud detection example must be coherent and well-explained.",
            "category": "Skill Based"
        },
        {
            "id": "8",
            "question": "Describe a project where you used Hugging Face's Transformers library to fine-tune a pre-trained language model for a specific NLP task (e.g., text classification, sentiment analysis). Explain the challenges you encountered during the fine-tuning process and the solutions you implemented to address them.",
            "template": "The candidate should describe a real-world project, highlighting the specific NLP task, the pre-trained model used, and the fine-tuning process. They should discuss challenges like overfitting, underfitting, computational resource limitations, and data scarcity. They should also mention strategies like hyperparameter tuning, regularization techniques, data augmentation, and transfer learning to overcome these challenges.",
            "criteria": "The answer should include a well-defined project with clear objectives. The candidate should demonstrate practical experience with the Hugging Face Transformers library and articulate specific challenges encountered and the solutions applied. The explanation should be detailed, technically sound, and demonstrate problem-solving skills.",
            "category": "Skill Based"
        },
        {
            "id": "9",
            "question": "During your LLM internship at We45, you led the development of 'ThreatPlaybook'. Imagine a scenario where, after the initial deployment, security analysts report that the generated threat scenarios, while technically accurate, often lack the specific context needed to be truly actionable for their environment. How would you approach refining ThreatPlaybook to better address this feedback, considering Databricks' focus on practical data solutions?",
            "template": "The candidate should demonstrate an understanding of the iterative development process for AI products. They should discuss methods for gathering feedback from security analysts, such as surveys or user interviews. They should propose concrete techniques for improving the contextual relevance of the generated scenarios, such as incorporating user-defined configuration options, integrating with existing security information and event management (SIEM) systems, or using a more sophisticated prompt engineering strategy to guide the LLM. The candidate should also mention metrics for evaluating the success of these refinements, such as analyst satisfaction scores or the time required to implement recommended mitigations.",
            "criteria": "The answer will be judged based on the candidate's ability to understand the problem, propose practical solutions, and demonstrate a customer-centric mindset. Strong answers will show an understanding of LLM limitations and mitigation techniques. The answer should also include consideration of how to measure the effectiveness of their proposed solutions and the role of Databricks in a data-driven refinement process.",
            "category": "Situation Based"
        },
        {
            "id": "10",
            "question": "In your role as a Research Assistant at Georgia Institute of Technology, you evaluated long-context LLMs like Llama 3.1 8B for question answering. Suppose a critical bug is discovered in Llama 3.1 8B that significantly impacts its ability to accurately answer questions related to specific sections of credit agreements. This vulnerability is causing widespread concern among financial institutions relying on this model. Describe how you would communicate this finding, its potential impact, and proposed mitigation strategies to both a technical audience (e.g., Databricks engineers) and a non-technical audience (e.g., stakeholders at a financial institution), considering the need for timely and accurate information dissemination.",
            "template": "The candidate should demonstrate strong communication and problem-solving skills. For the technical audience, they should discuss the specifics of the bug, its potential impact on model performance, and proposed solutions such as patching the model, implementing workarounds, or switching to a more robust alternative. For the non-technical audience, they should focus on the business impact of the bug, explaining how it could affect their operations and what steps are being taken to address the issue. The candidate should also emphasize the importance of transparency and collaboration in resolving the vulnerability.",
            "criteria": "The answer will be judged based on the candidate's ability to communicate technical information clearly and effectively to different audiences. Strong answers will demonstrate empathy for the concerns of both technical and non-technical stakeholders and highlight the importance of timely and transparent communication in crisis situations. The answer should also show an understanding of how a data scientist would collaborate with engineers to address the bug and the data-driven approach to mitigating the risk.",
            "category": "Situation Based"
        }
    ]
}