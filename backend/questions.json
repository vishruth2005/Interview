{
    "questions": [
        {
            "id": "1",
            "question": "Describe a scenario where you would choose TF-IDF over pre-trained word embeddings for feature extraction in a legal text classification task. What are the specific trade-offs in terms of computational cost, accuracy, and interpretability in this context?",
            "template": "The candidate should discuss the simplicity and speed of TF-IDF compared to the higher computational cost of pre-trained embeddings. They should mention that TF-IDF can be more interpretable, as it directly reflects word frequencies, while embeddings can capture semantic relationships but are less transparent. They should consider the size of the legal text corpus and the complexity of the classification task when making the decision, noting TF-IDF's suitability for smaller datasets and simpler tasks.",
            "criteria": "The answer should demonstrate an understanding of both TF-IDF and word embeddings, their strengths and weaknesses, and their applicability in a legal text context. The candidate should be able to articulate the trade-offs between them clearly and justify their choice based on specific scenario characteristics.",
            "category": "Project Based"
        },
        {
            "id": "2",
            "question": "Explain the concept of 'generative fallback' in a chatbot system like SheRise. What strategies did you employ to ensure contextual awareness was maintained when the Dialogflow system defaulted to the fallback intent, and why is this crucial for user experience in a sensitive domain like women's support?",
            "template": "The candidate should explain that generative fallback provides responses when the chatbot doesn't understand the user's intent. They should detail the methods used to maintain context, such as using conversation history or context variables in Dialogflow to generate relevant fallback responses. They should emphasize the importance of providing appropriate and supportive responses, even when the chatbot is uncertain, to avoid frustrating or alienating users seeking sensitive support. The explanation should include techniques to avoid generic or unhelpful answers during fallback.",
            "criteria": "The answer should demonstrate understanding of generative fallback and its importance in chatbot design, especially within sensitive applications. The candidate should provide concrete examples of how they maintained contextual awareness and generated helpful responses during fallback, while highlighting the user experience considerations.",
            "category": "Project Based"
        },
        {
            "id": "3",
            "question": "In your work with Agentic LLM workflows for ThreatPlaybook, how did you leverage the output of one agent to inform the actions or decisions of subsequent agents in the threat modeling process? Describe a specific example and explain the mechanisms you implemented to ensure data consistency and prevent error propagation across the agent chain.",
            "template": "The candidate should illustrate a sequential flow where the initial agent generates threat scenarios based on system vulnerabilities. The next agent then uses these scenarios to suggest mitigations, while a third agent analyzes the effectiveness of those mitigations. They should explain the importance of structured data formats (e.g., JSON) for seamless data transfer, error handling mechanisms (e.g., validation checks, retry logic), and consistency checks throughout the chain to ensure the integrity of the final threat model. Details of prompt engineering techniques to enforce structured output are valuable.",
            "criteria": "The response should showcase practical knowledge of agentic workflows and how to manage the interactions between agents effectively. The candidate should demonstrate a clear understanding of data consistency challenges and the methods used to address them. The depth and specificity of the example scenario are key indicators of practical experience.",
            "category": "Project Based"
        },
        {
            "id": "4",
            "question": "In your research at Georgia Tech, you built knowledge graphs using RDF with ontology and data layers. Describe a specific SPARQL query you designed to retrieve party information based on a complex relationship involving multiple entities in the credit agreement document. Explain how the ontology layer facilitated this query and what optimizations you considered for query performance on a large-scale knowledge graph.",
            "template": "The candidate should provide a concrete example of a SPARQL query, illustrating how it traverses the knowledge graph to extract the desired information. They should articulate the role of the ontology in defining the relationships between entities (e.g., 'Party' 'hasAgreement' 'CreditAgreement', 'CreditAgreement' 'signedBy' 'Party'). They should also discuss optimizations such as indexing, query rewriting, or the use of specialized graph databases to handle the scale and complexity of the knowledge graph. Specific examples of graph database technologies would be beneficial.",
            "criteria": "The answer should show a strong understanding of knowledge graph construction and querying using SPARQL. The quality and complexity of the example query are essential. The discussion of optimization techniques demonstrates practical experience with large-scale knowledge graphs.",
            "category": "Project Based"
        },
        {
            "id": "5",
            "question": "You surpassed the baseline model by 60% in Macro F1 score for the SemEval'24 legal text classification project using Weighted Multi-Level Fusion. Elaborate on the specific levels you fused (e.g., word, sentence, document) and the weighting scheme you implemented. How did you determine the optimal weights for each level, and what techniques did you use to prevent overfitting during the fusion process?",
            "template": "The candidate should detail the levels of fusion, providing examples of features used at each level (e.g., word embeddings at the word level, sentiment scores at the sentence level, metadata at the document level). They should describe the weighting scheme (e.g., learned weights, hand-tuned weights based on validation performance), and explain how they optimized the weights (e.g., using a grid search or optimization algorithm). To address overfitting, they should mention techniques like regularization, cross-validation, or early stopping. The methodology of validation set creation is important.",
            "criteria": "The answer should demonstrate a deep understanding of multi-level fusion and weighting techniques. The candidate should clearly explain the methodology used to determine optimal weights and prevent overfitting. The level of detail and the justification for the chosen methods are key indicators of the candidate's expertise.",
            "category": "Project Based"
        },
   
        {
            "id": "7",
            "question": "Given your experience with PyTorch and TensorFlow, how would you approach selecting the appropriate framework for a specific deep learning task within the Databricks environment, considering factors like scalability, integration with Spark, and model deployment?",
            "template": "The candidate should discuss factors like existing infrastructure (Databricks/Spark), the need for distributed training, the complexity of the model, and the ease of deployment using MLflow. They should demonstrate an understanding of the strengths and weaknesses of each framework in a large-scale data environment. Discuss the integration capabilities of each framework with Spark and Databricks ecosystem.",
            "criteria": "The answer should demonstrate an understanding of the practical considerations when choosing between PyTorch and TensorFlow for large-scale deep learning projects, particularly within the Databricks ecosystem. Correctly identifying factors like scalability, deployment ease, integration and available infrastructure are crucial.",
            "category": "Technical Skills"
        },
        {
            "id": "8",
            "question": "Explain how you would implement Retrieval-Augmented Generation (RAG) using Hugging Face Transformers and Databricks, considering the challenges of indexing and searching a large knowledge base. Detail the steps involved in building the RAG pipeline, including embedding generation, vector storage, and query routing.",
            "template": "The candidate should outline the process of creating a RAG pipeline. This includes: 1) Preprocessing the knowledge base, 2) Generating embeddings using Hugging Face models (e.g., Sentence Transformers), 3) Storing embeddings in a vector database (e.g., FAISS, Annoy) accessible from Databricks, 4) Implementing a search function to retrieve relevant documents based on a user query, and 5) Using a language model to generate the final response. They should address optimization strategies for large datasets within Databricks.",
            "criteria": "The response should highlight the key components of a RAG pipeline, focusing on its implementation within the Databricks environment. It should show knowledge of appropriate tools (Hugging Face, vector databases) and techniques for handling large-scale data. Practical considerations like latency and scalability should also be discussed.",
            "category": "Technical Skills"
        },
        {
            "id": "9",
            "question": "Describe your experience with unsupervised learning techniques. How would you leverage unsupervised learning for anomaly detection in a large dataset of network traffic logs stored in Delta Lake on Databricks? What metrics would you use to evaluate the performance of your anomaly detection model?",
            "template": "The candidate should explain relevant unsupervised learning algorithms (e.g., clustering, autoencoders, isolation forests) and how to apply them to anomaly detection. They should discuss the challenges of working with high-dimensional data and potential solutions like dimensionality reduction. They should mention metrics like precision, recall, F1-score (specifically when labelled data is used to evaluate) or silhouette score (if no labelled data is used) and how these can be used to assess the performance of the anomaly detection model. Additionally, they should describe how to integrate the model with Delta Lake for real-time anomaly detection.",
            "criteria": "The answer should showcase a strong understanding of unsupervised learning and its application to anomaly detection. It should clearly articulate the steps required to build and evaluate an anomaly detection model within the Databricks ecosystem, using Delta Lake and appropriate evaluation metrics. The choice of the algorithm and how its parameters will be optimized for performance are critical.",
            "category": "Technical Skills"
        },
        {
            "id": "10",
            "question": "Explain Transfer Learning and Few-Shot Learning. Provide a practical example of how you could apply these techniques to train a sentiment analysis model using a small, labeled dataset in Databricks, leveraging pre-trained language models from Hugging Face.",
            "template": "The candidate should define Transfer Learning and Few-Shot Learning, highlighting the advantages of leveraging pre-trained models. The response should outline the process of fine-tuning a pre-trained language model (e.g., BERT, RoBERTa) from Hugging Face on a small sentiment analysis dataset within Databricks. This would include loading the pre-trained model, adding a classification layer, training on the labeled dataset, and evaluating the model's performance. The explanation should focus on practical steps and challenges.",
            "criteria": "The response should correctly define Transfer Learning and Few-Shot Learning and demonstrate a clear understanding of how to apply them to a real-world problem. It should outline the key steps of fine-tuning a pre-trained language model using Hugging Face Transformers in Databricks, including data loading, model modification, training, and evaluation. The efficiency of this approach when limited data is available is crucial.",
            "category": "Technical Skills"
        },
        {
            "id": "11",
            "question": "Describe your experience with Language Models (LLMs). How would you fine-tune a pre-trained LLM for a specific text generation task, such as generating code documentation, using Databricks and MLflow for experiment tracking and model management?",
            "template": "The candidate should discuss the process of fine-tuning a pre-trained LLM (e.g., GPT-2, GPT-3) on a specific dataset. This includes: 1) Preparing the data, 2) Selecting an appropriate model architecture and hyperparameters, 3) Training the model using distributed training on Databricks, 4) Tracking experiments using MLflow, 5) Evaluating the model's performance using relevant metrics (e.g., perplexity, BLEU score), and 6) Deploying the model using MLflow's model management features. They should explain how to use MLflow to track different versions of the model and compare their performance.",
            "criteria": "The answer should demonstrate practical experience with fine-tuning LLMs and using MLflow for experiment tracking and model management within Databricks. The explanation should be detailed, covering all the necessary steps and considerations for a successful fine-tuning and deployment process. Understanding of the appropriate evaluation metrics for text generation tasks is also important.",
            "category": "Technical Skills"
        },
        {
            "id": "12",
            "question": "Explain the concept of Knowledge Graphs. How could you build a Knowledge Graph from unstructured text data using NLP techniques and store it in a graph database accessible from Databricks? What are the potential applications of such a Knowledge Graph within Databricks?",
            "template": "The candidate should define Knowledge Graphs and explain their structure (nodes and edges). They should describe the process of extracting entities and relationships from unstructured text using NLP techniques (e.g., Named Entity Recognition, Relation Extraction). They should then explain how to store the extracted information in a graph database (e.g., Neo4j, Amazon Neptune) accessible from Databricks. Finally, they should discuss potential applications, such as improved search, recommendation systems, and knowledge discovery within Databricks. Mentioning GraphFrames usage with Spark is a plus.",
            "criteria": "The answer should demonstrate a strong understanding of Knowledge Graphs, NLP techniques, and graph databases. It should clearly articulate the steps required to build a Knowledge Graph from unstructured text and discuss its potential applications within the Databricks ecosystem. The candidate should demonstrate a practical approach to applying these techniques to solve real-world problems.",
            "category": "Technical Skills"
        },
        {
            "id": "13",
            "question": "Describe your experience with distributed computing. How would you optimize a computationally intensive machine learning task (e.g., training a deep learning model) to run efficiently on a Databricks cluster using Spark? Discuss techniques for data partitioning, task distribution, and resource management.",
            "template": "The candidate should discuss techniques like data partitioning (e.g., using Spark's `repartition` or `coalesce` functions), task distribution (e.g., using Spark's `map` or `foreachPartition` functions), and resource management (e.g., configuring Spark's executors and memory settings). They should also discuss how to use Spark's caching mechanism to avoid recomputing intermediate results. Finally, the candidate should touch upon efficient ways to load and preprocess large datasets within Spark. They should discuss optimization techniques for large datasets.",
            "criteria": "The answer should demonstrate a strong understanding of distributed computing principles and how to apply them to optimize machine learning tasks on Databricks using Spark. The explanation should be detailed, covering all the key aspects of data partitioning, task distribution, and resource management. Knowledge of Spark's performance tuning techniques is essential.",
            "category": "Technical Skills"
        },
        {
            "id": "14",
            "question": "Explain the importance of Data Annotation in the context of machine learning. Describe different data annotation techniques and how you would manage a data annotation project to ensure high-quality annotations for training a machine learning model in Databricks.",
            "template": "The candidate should emphasize the impact of data annotation quality on the performance of machine learning models. The response should discuss different annotation techniques (e.g., bounding boxes, segmentation, text tagging) and tools. They should explain how to manage a data annotation project, including defining annotation guidelines, selecting annotators, monitoring annotation quality, and resolving disagreements. They should also discuss how to integrate the annotated data with Databricks for model training. Focus on the trade-offs between different annotation techniques. Should also talk about active learning for efficient data annotation.",
            "criteria": "The answer should demonstrate a clear understanding of the importance of data annotation and the best practices for managing a data annotation project. It should articulate the steps required to ensure high-quality annotations and integrate them with Databricks for model training. The candidate should demonstrate a practical approach to data annotation and a commitment to ensuring data quality.",
            "category": "Technical Skills"
        },
        {
            "id": "15",
            "question": "Describe your experience with SQL and different types of joins. Explain how you would optimize a complex SQL query that joins multiple large tables in Databricks SQL to improve its performance. What indexing strategies would you consider?",
            "template": "The candidate should demonstrate understanding of different join types (INNER, LEFT, RIGHT, FULL OUTER). The candidate should discuss query optimization techniques like using appropriate join types, filtering data early, avoiding unnecessary computations, and using indexes. They should also discuss how to use Databricks SQL's query execution plan to identify performance bottlenecks. Candidate should also mention Z-ordering and data skipping.",
            "criteria": "The answer should showcase a strong understanding of SQL and query optimization techniques. It should clearly articulate the steps required to optimize a complex SQL query in Databricks SQL, focusing on indexing strategies, join optimization, and query plan analysis. Practical experience with performance tuning is essential.",
            "category": "Technical Skills"
        },
        {
            "id": "16",
            "question": "What are Delta Lake's key features and benefits? How does Delta Lake enhance data reliability and performance compared to traditional data lakes? Describe a scenario where using Delta Lake in Databricks would be particularly advantageous, and explain why.",
            "template": "The candidate should highlight ACID transactions, schema enforcement, time travel, and data versioning as key features of Delta Lake. They should compare Delta Lake to traditional data lakes (e.g., Hadoop, S3) and explain how it improves data reliability and performance. The scenario described should demonstrate an understanding of Delta Lake's strengths and how it addresses common data lake challenges (e.g., data corruption, inconsistent data). The answer should include an explanation of the benefits of using Delta Lake in the given scenario.",
            "criteria": "The response should correctly identify and explain Delta Lake's key features and benefits. It should demonstrate a clear understanding of how Delta Lake enhances data reliability and performance compared to traditional data lakes. The scenario described should be realistic and demonstrate a practical understanding of Delta Lake's capabilities.",
            "category": "Technical Skills"
        },
        {
            "id": "17",
            "question": "Haricharana, your resume mentions leading the AI division in developing 'ThreatPlaybook' at We45. Tell me about a time when you faced a significant technical roadblock during the development of ThreatPlaybook or a similar project, and how you navigated the situation.",
            "template": "The candidate should describe a specific technical challenge encountered during the development process. The response should outline the steps taken to identify the problem, the resources consulted (e.g., documentation, colleagues, online forums), and the solution implemented. The candidate should demonstrate problem-solving skills, technical proficiency, and resourcefulness. They should also mention the impact of the solution on the project timeline or overall success. Emphasize the 'how' rather than just stating what the problem was. Be prepared to answer follow-up questions about the specific technologies or algorithms involved.",
            "criteria": "The answer should demonstrate a clear understanding of the technical problem, a structured approach to problem-solving, effective communication of technical details, and a focus on the impact of the solution. Look for evidence of analytical skills, resourcefulness, and the ability to learn and adapt. A good answer will show initiative and ownership of the problem-solving process. The clarity and the technical correctness of the approach are crucial.",
            "category": "Situational"
        },
        {
            "id": "18",
            "question": "Imagine you are working on a critical data analysis project for a major Databricks customer. Halfway through, you discover a fundamental flaw in the data pipeline that will require a complete re-engineering of the solution. The deadline is rapidly approaching. Describe how you would communicate this issue to your team and the client, and outline the steps you would take to address the situation and still deliver a valuable outcome.",
            "template": "The candidate should demonstrate strong communication skills, particularly in delivering bad news. They should outline a plan for transparently communicating the issue to both the team and the client. The response should emphasize honesty, empathy, and a proactive approach to finding a solution. The candidate should also describe their approach to prioritizing tasks, re-allocating resources, and potentially negotiating a revised scope or timeline with the client. They should emphasize maintaining the client's trust and providing alternative solutions or insights, even if the original deliverable cannot be met on time. Discussing risk mitigation strategies and lessons learned is also important.",
            "criteria": "The answer should demonstrate excellent communication skills, problem-solving abilities, client management skills, and a commitment to delivering value. Look for evidence of leadership, accountability, and the ability to remain calm and focused under pressure. A strong answer will show that the candidate can balance technical considerations with business needs and maintain positive relationships with stakeholders even in challenging situations.",
            "category": "Situational"
        }
    ]
}