{
    "questions": [
        {
            "id": "1",
            "question": "In the context of your Legal Argument Reasoning project, you used Weighted Multi-Level Fusion. Can you describe a scenario where a simple averaging of the levels would have been insufficient and why weighting was crucial for improving performance?",
            "template": "The candidate should be able to explain scenarios such as: where one level (e.g., document-level) contains significantly more noise or irrelevant information than another (e.g., sentence-level). Simple averaging would dilute the valuable information from the cleaner level. They should also discuss the mathematical justification for weighting, potentially mentioning techniques like gradient descent for optimizing weights based on a validation set.",
            "criteria": "The answer should demonstrate a clear understanding of the limitations of simple averaging, the benefits of weighting, and the ability to connect the approach to concrete scenarios in legal text classification. Bonus points for discussing weight optimization techniques.",
            "category": "Project Based"
        },
        {
            "id": "2",
            "question": "You've worked with both Haystack and Hugging Face Transformers for question answering. Compare and contrast their strengths and weaknesses, specifically focusing on their ease of use for dataset annotation and model fine-tuning in a low-resource NLP setting like your Sanskrit scripture project.",
            "template": "The candidate should compare Haystack's out-of-the-box components for QA pipelines (document stores, retrievers, readers) against the flexibility and model availability of Hugging Face Transformers. They should consider Haystack's annotation tool and how it simplifies dataset creation but might be less customizable. For low-resource scenarios, they should touch upon the effectiveness of transfer learning with pre-trained Transformers and how Haystack facilitates this, but also acknowledge the potential need for custom training loops with HF Transformers for more control.",
            "criteria": "The answer should showcase practical experience with both frameworks, a nuanced understanding of their trade-offs, and an appreciation for the specific challenges of low-resource NLP. The candidate should not simply list features but analyze their suitability for dataset annotation and model fine-tuning.",
            "category": "Project Based"
        },
        {
            "id": "3",
            "question": "In your LLM Intern role at We45, you implemented RAG using OWASP data to enhance vulnerability analysis in ThreatPlaybook. Describe a specific challenge you faced in integrating the OWASP data, such as data format inconsistencies, noise, or relevance filtering, and how you addressed it to ensure high-quality threat scenario generation.",
            "template": "The candidate should identify specific challenges related to OWASP data like its semi-structured nature, potential for outdated information, or the presence of irrelevant details for a particular vulnerability. They should then describe their approach, such as using regular expressions or parsing libraries to extract relevant information, implementing similarity metrics to filter out irrelevant entries, and potentially using data augmentation techniques to handle limited examples.",
            "criteria": "The answer should demonstrate practical experience with RAG, awareness of real-world data challenges, and the ability to apply appropriate data cleaning and filtering techniques to improve the quality of threat scenario generation. Bonus points for mentioning specific OWASP resources (e.g., OWASP Top Ten, OWASP Testing Guide) and how they were handled.",
            "category": "Project Based"
        },
        {
            "id": "4",
            "question": "You built a document-level question-answering system using RDF knowledge graphs at Georgia Tech. Explain how you designed the ontology for your credit agreement domain, specifically detailing the key entities, relationships, and reasoning capabilities you incorporated to enable multi-level query generation. Provide a concrete example of a complex query that your system could handle.",
            "template": "The candidate should describe the process of designing the ontology, identifying key entities (e.g., 'Party', 'Agreement', 'Clause', 'Obligation', 'Condition'), relationships between them (e.g., 'hasParty', 'governs', 'imposes'), and the reasoning rules (e.g., inferring the ultimate beneficiary of an obligation). They should also illustrate with a complex query, such as 'Which parties are responsible for fulfilling obligations related to specific clauses that are conditionally dependent on the occurrence of a particular event mentioned in the agreement?'",
            "criteria": "The response should showcase a deep understanding of ontology design, RDF, and knowledge graphs, as well as the ability to apply these concepts to a complex domain. The clarity and relevance of the ontology description and the complexity of the example query are key evaluation factors.",
            "category": "Project Based"
        },
        {
            "id": "5",
            "question": "In your publication, you mention using Multi-level Summarization for Legal Text in an Unsupervised Question Answering System. Describe the different levels of summarization you employed (e.g., sentence, paragraph, document), and how you ensured coherence and minimized redundancy across these levels to generate effective summaries that facilitated question answering.",
            "template": "The candidate should explain the different levels of summarization, potentially including techniques like extractive summarization at the sentence level (selecting important sentences), abstractive summarization at the paragraph level (rewriting and consolidating information), and topic modeling at the document level (identifying key themes). They should discuss methods for ensuring coherence, such as coreference resolution, and redundancy reduction, such as identifying and removing overlapping information across summaries. The candidate should also explain how these summaries are used within the QA system.",
            "criteria": "The answer should demonstrate a clear understanding of multi-level summarization techniques, coherence and redundancy challenges, and the ability to apply these concepts to the legal domain. Bonus points for discussing specific algorithms or libraries used for each summarization level and how the summarization process integrates with the question answering system.",
            "category": "Project Based"
        },
        {
            "id": "6",
            "question": "Explain how Retrieval-Augmented Generation (RAG) enhances vulnerability analysis, particularly in the context of using OWASP data.",
            "template": "The candidate should describe RAG as a process that combines retrieving relevant information from a knowledge source (OWASP data in this case) and using it to augment the generation of text by a language model. They should explain how this leads to more accurate, context-aware, and up-to-date vulnerability analysis compared to relying solely on the language model's pre-existing knowledge.",
            "criteria": "The answer should demonstrate understanding of RAG, its components, and its benefits in the specific context of vulnerability analysis and OWASP data. The candidate should also be able to articulate why and how RAG outperforms traditional methods.",
            "category": "Theory Based"
        },
        {
            "id": "7",
            "question": "What is the role of knowledge graphs in a document-level question-answering system, and how does using RDF with ontology and data layers contribute to multi-level query generation?",
            "template": "The candidate should explain that knowledge graphs provide a structured representation of information, enabling efficient querying and reasoning. They should then explain how RDF provides a standard format for representing the graph, and that the ontology and data layers provide a schema and the actual data, respectively. They should describe how this structured approach enables multi-level queries, allowing users to ask questions at different levels of abstraction.",
            "criteria": "The answer should demonstrate a clear understanding of knowledge graphs, RDF, ontologies, and data layers. The candidate should also articulate how these components work together to enable effective question answering and multi-level query generation.",
            "category": "Theory Based"
        },
        {
            "id": "8",
            "question": "Describe the relationship between transfer learning and few-shot learning. How does transfer learning facilitate few-shot learning in low-resource NLP scenarios like the Sanskrit scriptures project?",
            "template": "The candidate should explain that transfer learning is a technique where knowledge gained from solving one problem is applied to a different but related problem, and that few-shot learning is a machine learning approach that aims to learn effectively from a limited amount of training data. They should then describe how transfer learning, by leveraging pre-trained models on large datasets, can provide a strong starting point for few-shot learning scenarios where labeled data is scarce, such as in low-resource NLP for Sanskrit scriptures.",
            "criteria": "The answer should accurately define both transfer learning and few-shot learning and clearly articulate how transfer learning provides a crucial advantage in few-shot learning scenarios, particularly in the context of low-resource NLP. The candidate should connect these concepts to the specifics of the Sanskrit scriptures project mentioned in the resume.",
            "category": "Theory Based"
        },
        {
            "id": "9",
            "question": "Explain the difference between eager execution and graph mode in TensorFlow. What are the advantages and disadvantages of each, and when would you choose one over the other?",
            "template": "The candidate should explain that eager execution runs operations immediately, making debugging easier and code more Pythonic. Graph mode, on the other hand, constructs a computational graph first, allowing for optimizations and deployment to different platforms. They should discuss the trade-offs between ease of use and performance.",
            "criteria": "The answer should demonstrate a clear understanding of the operational differences between eager execution and graph mode. The candidate should articulate the pros and cons of each mode and provide relevant use-case scenarios where one would be preferred over the other. The depth of understanding of TensorFlow's execution model will be assessed.",
            "category": "Skill Based"
        },
        {
            "id": "10",
            "question": "Describe a scenario where you would use PyTorch Lightning instead of standard PyTorch. What benefits does PyTorch Lightning provide, and what are its limitations?",
            "template": "The candidate should discuss how PyTorch Lightning simplifies the training loop by abstracting away boilerplate code. They should mention features like automatic mixed precision (AMP), distributed training, and early stopping. They should also acknowledge situations where Lightning might be too restrictive or add unnecessary complexity.",
            "criteria": "The candidate's response should highlight the advantages of using PyTorch Lightning for streamlining the training process. The answer should include specific benefits and potential drawbacks or situations where the additional abstraction layer might be counterproductive. The understanding of PyTorch's training process and the role of PyTorch Lightning in simplifying it will be evaluated.",
            "category": "Skill Based"
        },
        {
            "id": "11",
            "question": "Explain the concept of attention mechanisms in NLP. How do they improve upon traditional sequence-to-sequence models, and can you provide an example of how you've used attention in a project?",
            "template": "The candidate should explain that attention mechanisms allow the model to focus on relevant parts of the input sequence when generating the output. They should highlight how this addresses the limitations of fixed-length context vectors in traditional sequence-to-sequence models. An example project involving machine translation or text summarization would be ideal.",
            "criteria": "The answer should showcase a clear understanding of attention mechanisms and their role in improving NLP models. The candidate should be able to explain the theoretical benefits and provide a practical example from their own experience, demonstrating their ability to apply this knowledge in a real-world scenario. The understanding of sequence-to-sequence models and their limitations will be assessed.",
            "category": "Skill Based"
        },
        {
            "id": "12",
            "question": "Describe the key differences between BERT and Transformer XL. In what scenarios would you prefer one over the other, and why?",
            "template": "The candidate should explain that BERT uses a masked language model and a next sentence prediction task for pre-training, while Transformer XL addresses the limitations of fixed-length context by incorporating a recurrence mechanism. They should discuss that Transformer XL is better for longer sequences while BERT is often faster to train and fine-tune for tasks with short/medium sequences.",
            "criteria": "The response should indicate a strong understanding of the architectural differences between BERT and Transformer XL. The candidate should articulate the trade-offs between them and explain which model is best suited for different sequence lengths. The understanding of the inner-workings and the relative performance characteristics of each model will be evaluated.",
            "category": "Skill Based"
        },
        {
            "id": "13",
            "question": "Explain the concept of Retrieval-Augmented Generation (RAG). How does it work, and what are its advantages compared to fine-tuning a language model directly?",
            "template": "The candidate should describe RAG as a process where a language model retrieves relevant information from an external knowledge source and uses it to generate more informed responses. They should discuss how RAG allows the model to leverage up-to-date information without requiring constant retraining.",
            "criteria": "The response should thoroughly explain the RAG framework, including the retrieval and generation steps. The candidate should articulate the key advantages of RAG, such as its ability to incorporate new information without retraining the language model, along with any limitations it might have. Understanding of the trade-offs between fine-tuning and retrieval-augmentation will be assessed.",
            "category": "Skill Based"
        },
        {
            "id": "14",
            "question": "How would you use pandas to efficiently process a large CSV file (e.g., several gigabytes) that exceeds your machine's memory?",
            "template": "The candidate should suggest using chunking (e.g., `pd.read_csv(..., chunksize=...)`) to process the file in smaller, manageable pieces. They should also mention techniques for optimizing memory usage, such as specifying data types, using categorical data where appropriate, and avoiding unnecessary data copies.",
            "criteria": "The response should demonstrate practical knowledge of how to handle large datasets in pandas. The candidate should suggest appropriate techniques for processing the data in chunks and optimizing memory usage. Emphasis will be placed on efficiency and the ability to work with datasets that exceed available memory.",
            "category": "Skill Based"
        },
        {
            "id": "15",
            "question": "You have a dataset with highly imbalanced classes. What strategies would you use to train a machine learning model on this data, and how would you evaluate its performance?",
            "template": "The candidate should discuss techniques like oversampling the minority class (e.g., SMOTE), undersampling the majority class, using cost-sensitive learning, and employing appropriate evaluation metrics such as precision, recall, F1-score, and AUC-ROC.",
            "criteria": "The candidate's response should provide a comprehensive overview of strategies for handling imbalanced datasets. They should articulate the pros and cons of each technique and justify their choice based on the specifics of the dataset and the task at hand. The understanding of evaluation metrics beyond accuracy for imbalanced datasets will be assessed.",
            "category": "Skill Based"
        },
        {
            "id": "16",
            "question": "Explain the difference between supervised, unsupervised, and reinforcement learning. Give a practical example of when you would use each type of learning.",
            "template": "The candidate should clearly define each type of learning and provide examples that illustrate their applications. Supervised learning involves training a model on labeled data (e.g., image classification), unsupervised learning involves finding patterns in unlabeled data (e.g., clustering customer segments), and reinforcement learning involves training an agent to make decisions in an environment to maximize a reward (e.g., training a game-playing AI).",
            "criteria": "The answer should demonstrate a solid understanding of the fundamental differences between the three main types of machine learning. The examples should be practical and relevant, showing the candidate's ability to apply the correct type of learning to different problem domains. The clarity and accuracy of the definitions and examples will be evaluated.",
            "category": "Skill Based"
        },
        {
            "id": "17",
            "question": "Describe a time when you had to debug a complex machine learning model. What steps did you take to identify the issue, and how did you resolve it?",
            "template": "The candidate should provide a specific example and outline their debugging process. This might involve analyzing training and validation curves, examining feature importance, inspecting data samples, using debugging tools, and trying different model configurations or hyperparameters. The candidate should also mention any challenges they faced and how they overcame them.",
            "criteria": "The response should provide a structured and logical account of the debugging process. The candidate should demonstrate problem-solving skills, attention to detail, and the ability to use debugging tools and techniques effectively. The clarity of the explanation and the specific steps taken to resolve the issue will be evaluated.",
            "category": "Skill Based"
        },
        {
            "id": "18",
            "question": "How would you design a knowledge graph for a specific domain (e.g., scientific publications, e-commerce products)? What considerations would you take into account when choosing the entities, relationships, and properties to include in the graph?",
            "template": "The candidate should describe the process of defining the scope of the knowledge graph, identifying the key entities and relationships within the chosen domain, and selecting relevant properties for each entity. They should also discuss considerations such as data availability, scalability, and the intended use cases of the knowledge graph.",
            "criteria": "The answer should demonstrate an understanding of the principles of knowledge graph design. The candidate should be able to articulate a coherent and well-reasoned approach to building a knowledge graph for a specific domain, considering factors such as data quality, scalability, and the target applications. The ability to translate domain knowledge into a structured knowledge graph will be assessed.",
            "category": "Skill Based"
        },
        {
            "id": "19",
            "question": "During your LLM internship at We45, you led the AI division in developing 'ThreatPlaybook.' Imagine a scenario where, halfway through the project, the stakeholders request a significant change in the project's scope, demanding the integration of a completely new type of security threat analysis that was not originally planned. This change would require significant modifications to the existing architecture and potentially delay the project timeline. How would you approach this situation to ensure the project's successful completion while managing stakeholder expectations and the team's workload?",
            "template": "The candidate should demonstrate a structured approach to handling the change request, including:\n\n1.  **Assessment:** Immediately assess the impact of the new requirement on the project's timeline, resources, and existing architecture.\n2.  **Communication:** Communicate proactively with the stakeholders to understand the rationale behind the change and to discuss the potential implications.\n3.  **Planning:** Develop a revised project plan that incorporates the new requirement, outlining the necessary modifications, resource allocation, and timeline adjustments.\n4.  **Prioritization:** Clearly define the priorities and trade-offs involved in implementing the change, ensuring that critical functionalities are not compromised.\n5.  **Execution:** Effectively communicate the revised plan to the team, delegating tasks, and providing support to ensure smooth execution.\n6.  **Risk Management:** Identify and mitigate potential risks associated with the change, such as technical challenges, resource constraints, and timeline delays.\n7.  **Feedback Loop:** Maintain a continuous feedback loop with the stakeholders to keep them informed of the progress and to address any concerns or questions.",
            "criteria": "The candidate's response should be evaluated based on the following criteria:\n\n*   **Proactive Communication:** Did the candidate emphasize open and transparent communication with stakeholders?\n*   **Strategic Thinking:** Did the candidate demonstrate the ability to assess the impact of the change on the project's overall goals?\n*   **Problem-Solving Skills:** Did the candidate provide a clear and structured approach to managing the change and mitigating potential risks?\n*   **Team Leadership:** Did the candidate show an understanding of how to effectively communicate the change to the team and delegate tasks accordingly?\n*   **Stakeholder Management:** Did the candidate demonstrate the ability to manage stakeholder expectations and address any concerns or questions?",
            "category": "Situation Based"
        },
        {
            "id": "20",
            "question": "In your role as a Research Assistant at Georgia Institute of Technology, you are evaluating long-context LLMs for document-level question answering. Suppose you encounter a situation where Llama 3.1 8B consistently provides irrelevant answers, even with a well-constructed knowledge graph and optimized queries. Other team members suggest switching to a different model that might be more suitable, but that model would require a significantly larger computational budget. Given the constraints, what steps would you take to thoroughly investigate and attempt to resolve the issue with Llama 3.1 8B before considering a more expensive alternative?",
            "template": "The candidate should outline a systematic approach to troubleshooting the issue with Llama 3.1 8B:\n\n1.  **Query Analysis:** Scrutinize the queries being used. Ensure they are accurately formulated and aligned with the knowledge graph structure and the document content.\n2.  **Knowledge Graph Review:** Verify the integrity and completeness of the knowledge graph. Ensure all relevant information is correctly represented and accessible.\n3.  **RAG Pipeline Examination:** Investigate the Retrieval-Augmented Generation (RAG) pipeline. Check the relevance of retrieved contexts and the quality of the generated answers.\n4.  **Prompt Engineering:** Experiment with different prompting techniques to guide Llama 3.1 8B towards more relevant responses. Consider few-shot learning or chain-of-thought prompting.\n5.  **Fine-Tuning (Optional):** If feasible within the computational budget, consider fine-tuning Llama 3.1 8B on a relevant dataset to improve its performance on document-level question answering.\n6.  **Error Analysis:** Analyze the types of questions where Llama 3.1 8B consistently fails. Identify patterns or common themes in the errors.\n7.  **Comparative Analysis:** Compare the performance of Llama 3.1 8B with a smaller, more efficient model on a subset of questions. This will help determine if the issue is specific to the model's size or architecture.\n8.  **Collaboration:** Consult with other researchers or experts in the field to seek their insights and guidance.",
            "criteria": "The candidate's response should be evaluated based on the following criteria:\n\n*   **Problem-Solving Methodology:** Did the candidate demonstrate a systematic and thorough approach to troubleshooting the issue?\n*   **Technical Expertise:** Did the candidate demonstrate a strong understanding of LLMs, knowledge graphs, and RAG pipelines?\n*   **Resourcefulness:** Did the candidate explore various techniques to improve the performance of Llama 3.1 8B before resorting to a more expensive alternative?\n*   **Analytical Skills:** Did the candidate emphasize the importance of analyzing the queries, knowledge graph, and RAG pipeline to identify the root cause of the problem?\n*   **Collaboration and Communication:** Did the candidate show a willingness to seek help from others and collaborate with experts in the field?",
            "category": "Situation Based"
        }
    ]
}